{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "004_mnist_fashion.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:26.739189Z",
          "start_time": "2020-03-21T13:31:16.311066Z"
        },
        "id": "myRLFopGTGo2",
        "colab_type": "code",
        "outputId": "dad5c5b1-fbec-4ba9-fcf8-86c8e1ac79b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "!pip install --upgrade keras\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/53/e18c5e7a2263d3581a979645a185804782e59b8e13f42b9c3c3cfb5bb503/tensorflow_gpu-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (348.9MB)\n",
            "\u001b[K     |████████████████████████████████| 348.9MB 45kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.34.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.24.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 44.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.1)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.18.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.10.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.9.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 61.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (46.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n",
            "Collecting keras\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ad/fd/6bfe87920d7f4fd475acd28500a42482b6b84479832bdc0fe9e589a60ceb/Keras-2.3.1-py2.py3-none-any.whl (377kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 1.7MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n",
            "Requirement already satisfied, skipping upgrade: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.18.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n",
            "Requirement already satisfied, skipping upgrade: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.5\n",
            "    Uninstalling Keras-2.2.5:\n",
            "      Successfully uninstalled Keras-2.2.5\n",
            "Successfully installed keras-2.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iTpksCcib8uz",
        "colab_type": "code",
        "outputId": "4d9f4398-37a1-4d13-d9bd-d45f1560532a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        }
      },
      "source": [
        "pip install keras-rectified-adam"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras-rectified-adam\n",
            "  Downloading https://files.pythonhosted.org/packages/21/79/9521f66b92186702cb58a214c1b923b416266381cd824e15a1733f6a5b06/keras-rectified-adam-0.17.0.tar.gz\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (1.18.2)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras-rectified-adam) (2.3.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.4.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (2.8.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (3.13)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from Keras->keras-rectified-adam) (1.0.8)\n",
            "Building wheels for collected packages: keras-rectified-adam\n",
            "  Building wheel for keras-rectified-adam (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-rectified-adam: filename=keras_rectified_adam-0.17.0-cp36-none-any.whl size=14781 sha256=48b685c5dac37920543db6072c7b9e82a80946b023675bd3280d9dffe48acffd\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/01/27/3a934e1a5644f5b93c720422a6ef97034ea78a21ba71cfb549\n",
            "Successfully built keras-rectified-adam\n",
            "Installing collected packages: keras-rectified-adam\n",
            "Successfully installed keras-rectified-adam-0.17.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:26.833938Z",
          "start_time": "2020-03-21T13:31:26.742181Z"
        },
        "id": "iIv8z1zPTGo9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.layers import Conv2D\n",
        "from tensorflow.keras.layers import MaxPooling2D\n",
        "from tensorflow.keras.layers import GlobalMaxPooling2D\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.optimizers import SGD, Adam, RMSprop\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "# from keras_radam import RAdam    # try"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7mxgDg6TGpE",
        "colab_type": "text"
      },
      "source": [
        "# 1. 讀入 Fashion MNSIT 數據集"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:26.854880Z",
          "start_time": "2020-03-21T13:31:26.838923Z"
        },
        "id": "zhYXYn9hTGpF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:27.586922Z",
          "start_time": "2020-03-21T13:31:26.858871Z"
        },
        "id": "7TO-kMIETGpJ",
        "colab_type": "code",
        "outputId": "b48ba1ec-8f82-4f83-d240-a103f8e92a17",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "8192/5148 [===============================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T12:43:04.120678Z",
          "start_time": "2020-03-21T12:43:04.116686Z"
        },
        "id": "GjUpGt0OTGpP",
        "colab_type": "code",
        "outputId": "a0901023-d5bd-46dd-e266-83bc2471a842",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 看一下shape\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n",
            "(10000, 28, 28)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T12:52:06.063481Z",
          "start_time": "2020-03-21T12:52:06.051513Z"
        },
        "id": "6bCedLR7TGpU",
        "colab_type": "code",
        "outputId": "e6905436-19e7-441a-e32a-86e500e73b04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xdDOU6ETGpY",
        "colab_type": "text"
      },
      "source": [
        "# 2. 欣賞數據集內容"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:27.597894Z",
          "start_time": "2020-03-21T13:31:27.592909Z"
        },
        "id": "2fc2xbkCTGpZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
        "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:27.888118Z",
          "start_time": "2020-03-21T13:31:27.600886Z"
        },
        "id": "hdHGTj51TGpc",
        "colab_type": "code",
        "outputId": "0fb75b01-c036-482c-f4e7-5b9d9d7cc87c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "n = 1234\n",
        "print('這是', class_names[y_train[n]])\n",
        "plt.imshow(x_train[n], cmap='Greys');"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "這是 Bag\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkvg/rocmclU7RlYAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:28.155403Z",
          "start_time": "2020-03-21T13:31:27.891111Z"
        },
        "id": "DbpKWizqTGpg",
        "colab_type": "code",
        "outputId": "536b4b0a-3c74-40cc-cdb9-5ca626cd01c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "source": [
        "pick = np.random.choice(60000, 5, replace=False)    # 抽出不放回\n",
        "\n",
        "for i in range(5):\n",
        "    n = pick[i]\n",
        "    ax = plt.subplot(151+i)\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_title(class_names[y_train[n]], fontsize=10)\n",
        "    plt.imshow(x_train[n], cmap='Greys')  "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAABZCAYAAAAAY/6dAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nf51bwjMmnDHLAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:28.167372Z",
          "start_time": "2020-03-21T13:31:28.159393Z"
        },
        "id": "y4LwsnmGTGpm",
        "colab_type": "code",
        "outputId": "36af4bed-f39a-4f8f-eeb4-157e7ee1f3f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pick"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([22364, 45959, 38357, 28730, 55321])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hkdhJK6TGpp",
        "colab_type": "text"
      },
      "source": [
        "## 前處理"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:28.636119Z",
          "start_time": "2020-03-21T13:31:28.170364Z"
        },
        "id": "On3YzOfhTGpq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# parameters\n",
        "class_nums = len(class_names)\n",
        "\n",
        "\n",
        "# flatten and normalize\n",
        "x_train = x_train.reshape(60000, 784) / 255.\n",
        "x_test = x_test.reshape(10000, 784) / 255.\n",
        "\n",
        "x_train_original = x_train.reshape(60000, 28, 28)\n",
        "x_test_original = x_test.reshape(10000, 28, 28)\n",
        "\n",
        "# for CNN \n",
        "x_train_original = tf.expand_dims(x_train_original, axis=3)\n",
        "x_test_original = tf.expand_dims(x_test_original, axis=3)\n",
        "\n",
        "\n",
        "# one-hot y-data\n",
        "y_train = tf.keras.utils.to_categorical(y_train, class_nums)\n",
        "y_test = tf.keras.utils.to_categorical(y_test, class_nums)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GhYm-_LpTGpt",
        "colab_type": "text"
      },
      "source": [
        "## Model\n",
        "- 預計嘗試流程\n",
        "\n",
        "- 全連接 3 層\n",
        "    - optimizer(簡單嘗試, 先不考慮動量以及其他)\n",
        "        - SGD\n",
        "        - Adam\n",
        "        - RAdam\n",
        "    - initializer\n",
        "        - default 與 uniform在此例差不多\n",
        "    - which first (BN or dropout?)\n",
        "        - It doesn't matter.\n",
        "    - which first ? (BN or Activation function)\n",
        "        - \n",
        "    - early-stop\n",
        "    - another\n",
        "        - neurons\n",
        "            - not yey\n",
        "        - layers\n",
        "            - three layers are almost equal four layers\n",
        "        - batches\n",
        "        - lr\n",
        "            - not yey\n",
        "\n",
        "- CNN model\n",
        "    - optimizer\n",
        "    - initializer\n",
        "    - early-stop\n",
        "    - maybe data augmentation or transfering learnging\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tj3w7tqSde9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 簡單包裝讓model輕便一些\n",
        "\n",
        "def build_model(class_nums=10, input_shape=(784, ),layers=3, dropout=False, bn=False, init='glorot_uniform'):\n",
        "    model = Sequential()\n",
        "    neurons = 256\n",
        "    for i in range(layers-1):\n",
        "        if i == 0:\n",
        "            model.add(Dense(neurons, input_shape=input_shape, kernel_initializer=init))\n",
        "        else:\n",
        "            model.add(Dense(neurons, use_bias=True, kernel_initializer=init))\n",
        "        if dropout:\n",
        "            model.add(Dropout(0.25))\n",
        "        if bn:\n",
        "            model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        if neurons > 32:\n",
        "            neurons = 256 // 2\n",
        "    model.add(Dense(class_nums, activation='softmax'))\n",
        "    \n",
        "    # show model\n",
        "    model.summary()\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJu1fhvMiWi7",
        "colab_type": "text"
      },
      "source": [
        "# 測試一下, BN先還是activate先好?\n",
        "    - 猜測應該是BN先\n",
        "        - 因為先把資料標準化再activate比較合理\n",
        "    - answer:\n",
        "        - BN先效果比較好!!!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y--2ecfXioyq",
        "colab_type": "code",
        "outputId": "aa349285-962e-42e4-e495-885d2264bd54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Activation first\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(256, input_shape=(784, )))\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dense(128))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dense(class_nums, activation=\"softmax\")) \n",
        "\n",
        "# BN first\n",
        "model2 = Sequential()\n",
        "model2.add(Dense(256, input_shape=(784, )))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dense(128))\n",
        "model2.add(BatchNormalization())\n",
        "model2.add(Activation('relu'))\n",
        "model2.add(Dense(class_nums, activation=\"softmax\")) \n",
        "\n",
        "# see the model struture\n",
        "model.summary()\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "# train the model\n",
        "# documentation    :    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n",
        "\n",
        "model2.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model2.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_12 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,170\n",
            "Trainable params: 235,658\n",
            "Non-trainable params: 512\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 76us/sample - loss: 0.0294 - accuracy: 0.8010 - val_loss: 0.0238 - val_accuracy: 0.8383\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0227 - accuracy: 0.8445 - val_loss: 0.0248 - val_accuracy: 0.8296\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0212 - accuracy: 0.8546 - val_loss: 0.0224 - val_accuracy: 0.8507\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0205 - accuracy: 0.8607 - val_loss: 0.0226 - val_accuracy: 0.8438\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0201 - accuracy: 0.8631 - val_loss: 0.0215 - val_accuracy: 0.8526\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0193 - accuracy: 0.8688 - val_loss: 0.0209 - val_accuracy: 0.8602\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0191 - accuracy: 0.8693 - val_loss: 0.0242 - val_accuracy: 0.8395\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0188 - accuracy: 0.8740 - val_loss: 0.0211 - val_accuracy: 0.8571\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 4s 73us/sample - loss: 0.0186 - accuracy: 0.8740 - val_loss: 0.0210 - val_accuracy: 0.8560\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0181 - accuracy: 0.8783 - val_loss: 0.0213 - val_accuracy: 0.8599\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0179 - accuracy: 0.8806 - val_loss: 0.0214 - val_accuracy: 0.8542\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0176 - accuracy: 0.8814 - val_loss: 0.0212 - val_accuracy: 0.8612\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0174 - accuracy: 0.8829 - val_loss: 0.0215 - val_accuracy: 0.8605\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0176 - accuracy: 0.8822 - val_loss: 0.0225 - val_accuracy: 0.8484\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0173 - accuracy: 0.8839 - val_loss: 0.0195 - val_accuracy: 0.8700\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 4s 72us/sample - loss: 0.0171 - accuracy: 0.8859 - val_loss: 0.0260 - val_accuracy: 0.8284\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 4s 70us/sample - loss: 0.0170 - accuracy: 0.8875 - val_loss: 0.0211 - val_accuracy: 0.8610\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 4s 74us/sample - loss: 0.0167 - accuracy: 0.8877 - val_loss: 0.0226 - val_accuracy: 0.8509\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0170 - accuracy: 0.8875 - val_loss: 0.0197 - val_accuracy: 0.8708\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 4s 71us/sample - loss: 0.0168 - accuracy: 0.8892 - val_loss: 0.0205 - val_accuracy: 0.8645\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 5s 85us/sample - loss: 0.0258 - accuracy: 0.8194 - val_loss: 0.0227 - val_accuracy: 0.8444\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.0201 - accuracy: 0.8613 - val_loss: 0.0204 - val_accuracy: 0.8553\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0185 - accuracy: 0.8734 - val_loss: 0.0202 - val_accuracy: 0.8609\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0175 - accuracy: 0.8795 - val_loss: 0.0211 - val_accuracy: 0.8560\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0165 - accuracy: 0.8878 - val_loss: 0.0182 - val_accuracy: 0.8744\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0157 - accuracy: 0.8922 - val_loss: 0.0206 - val_accuracy: 0.8611\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0152 - accuracy: 0.8961 - val_loss: 0.0184 - val_accuracy: 0.8740\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.0148 - accuracy: 0.8990 - val_loss: 0.0183 - val_accuracy: 0.8771\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0139 - accuracy: 0.9057 - val_loss: 0.0170 - val_accuracy: 0.8834\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 5s 81us/sample - loss: 0.0136 - accuracy: 0.9089 - val_loss: 0.0178 - val_accuracy: 0.8778\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0132 - accuracy: 0.9107 - val_loss: 0.0182 - val_accuracy: 0.8741\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0128 - accuracy: 0.9137 - val_loss: 0.0182 - val_accuracy: 0.8768\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 5s 84us/sample - loss: 0.0125 - accuracy: 0.9172 - val_loss: 0.0174 - val_accuracy: 0.8824\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.0123 - accuracy: 0.9176 - val_loss: 0.0193 - val_accuracy: 0.8733\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 5s 80us/sample - loss: 0.0118 - accuracy: 0.9214 - val_loss: 0.0189 - val_accuracy: 0.8761\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0115 - accuracy: 0.9236 - val_loss: 0.0191 - val_accuracy: 0.8707\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 5s 82us/sample - loss: 0.0113 - accuracy: 0.9251 - val_loss: 0.0178 - val_accuracy: 0.8821\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 5s 78us/sample - loss: 0.0112 - accuracy: 0.9264 - val_loss: 0.0170 - val_accuracy: 0.8900\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 5s 86us/sample - loss: 0.0108 - accuracy: 0.9297 - val_loss: 0.0190 - val_accuracy: 0.8791\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 5s 79us/sample - loss: 0.0107 - accuracy: 0.9297 - val_loss: 0.0179 - val_accuracy: 0.8840\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4ee2441d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hoemUGuGmPn1",
        "colab_type": "text"
      },
      "source": [
        "## 測試優化器\n",
        "- 排名\n",
        "    - "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:02:40.074984Z",
          "start_time": "2020-03-21T13:01:38.047436Z"
        },
        "scrolled": true,
        "id": "I5r7htmqTGpu",
        "colab_type": "code",
        "outputId": "490eee69-b729-4841-ce23-b0a8269cf1a4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Adam is much better than SGD most of time.\n",
        "\n",
        "# build model\n",
        "# full-connectected\n",
        "model_Adam = build_model(dropout=True, bn=True)\n",
        "model_SGD = build_model(dropout=True, bn=True)\n",
        "model_RMSprop = build_model(dropout=True, bn=True)\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "# train the model\n",
        "# documentation    :    keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model_Adam.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model_Adam.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n",
        "\n",
        "print('==='*20)\n",
        "\n",
        "model_SGD.compile(optimizer=SGD(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model_SGD.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n",
        "\n",
        "print('==='*20)\n",
        "\n",
        "model_RMSprop.compile(optimizer=RMSprop(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model_RMSprop.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_41 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_25 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_15 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_26 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_24 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_44 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_16 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_27 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_25 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_17 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_28 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_26 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_47 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_18 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_29 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_27 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_19 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_30 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_28 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_49 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0283 - accuracy: 0.8015 - val_loss: 0.0271 - val_accuracy: 0.8125\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0231 - accuracy: 0.8406 - val_loss: 0.0245 - val_accuracy: 0.8283\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0218 - accuracy: 0.8499 - val_loss: 0.0235 - val_accuracy: 0.8418\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0208 - accuracy: 0.8563 - val_loss: 0.0213 - val_accuracy: 0.8531\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0200 - accuracy: 0.8634 - val_loss: 0.0191 - val_accuracy: 0.8703\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0193 - accuracy: 0.8681 - val_loss: 0.0212 - val_accuracy: 0.8521\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0189 - accuracy: 0.8709 - val_loss: 0.0239 - val_accuracy: 0.8285\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0185 - accuracy: 0.8732 - val_loss: 0.0194 - val_accuracy: 0.8669\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0179 - accuracy: 0.8783 - val_loss: 0.0185 - val_accuracy: 0.8724\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0177 - accuracy: 0.8792 - val_loss: 0.0193 - val_accuracy: 0.8676\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0175 - accuracy: 0.8814 - val_loss: 0.0185 - val_accuracy: 0.8747\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.0171 - accuracy: 0.8841 - val_loss: 0.0191 - val_accuracy: 0.8715\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0169 - accuracy: 0.8848 - val_loss: 0.0206 - val_accuracy: 0.8595\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0167 - accuracy: 0.8873 - val_loss: 0.0214 - val_accuracy: 0.8536\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0163 - accuracy: 0.8900 - val_loss: 0.0181 - val_accuracy: 0.8764\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0161 - accuracy: 0.8909 - val_loss: 0.0188 - val_accuracy: 0.8689\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0159 - accuracy: 0.8924 - val_loss: 0.0176 - val_accuracy: 0.8810\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0157 - accuracy: 0.8940 - val_loss: 0.0175 - val_accuracy: 0.8810\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0156 - accuracy: 0.8951 - val_loss: 0.0183 - val_accuracy: 0.8764\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0155 - accuracy: 0.8958 - val_loss: 0.0177 - val_accuracy: 0.8792\n",
            "============================================================\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 6s 104us/sample - loss: 0.0902 - accuracy: 0.1963 - val_loss: 0.0724 - val_accuracy: 0.4303\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 6s 101us/sample - loss: 0.0683 - accuracy: 0.4674 - val_loss: 0.0532 - val_accuracy: 0.6445\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0546 - accuracy: 0.6145 - val_loss: 0.0454 - val_accuracy: 0.7073\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 6s 102us/sample - loss: 0.0483 - accuracy: 0.6709 - val_loss: 0.0405 - val_accuracy: 0.7418\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0441 - accuracy: 0.7023 - val_loss: 0.0372 - val_accuracy: 0.7572\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0410 - accuracy: 0.7240 - val_loss: 0.0350 - val_accuracy: 0.7687\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0388 - accuracy: 0.7381 - val_loss: 0.0333 - val_accuracy: 0.7761\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0370 - accuracy: 0.7507 - val_loss: 0.0319 - val_accuracy: 0.7834\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0356 - accuracy: 0.7594 - val_loss: 0.0309 - val_accuracy: 0.7892\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0345 - accuracy: 0.7681 - val_loss: 0.0300 - val_accuracy: 0.7939\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 6s 94us/sample - loss: 0.0335 - accuracy: 0.7749 - val_loss: 0.0292 - val_accuracy: 0.7992\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0327 - accuracy: 0.7782 - val_loss: 0.0286 - val_accuracy: 0.8029\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 6s 100us/sample - loss: 0.0319 - accuracy: 0.7847 - val_loss: 0.0281 - val_accuracy: 0.8051\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0313 - accuracy: 0.7861 - val_loss: 0.0276 - val_accuracy: 0.8083\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0308 - accuracy: 0.7907 - val_loss: 0.0272 - val_accuracy: 0.8104\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 6s 98us/sample - loss: 0.0303 - accuracy: 0.7942 - val_loss: 0.0268 - val_accuracy: 0.8131\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0298 - accuracy: 0.7962 - val_loss: 0.0265 - val_accuracy: 0.8132\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.0292 - accuracy: 0.8025 - val_loss: 0.0262 - val_accuracy: 0.8148\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 6s 96us/sample - loss: 0.0290 - accuracy: 0.8017 - val_loss: 0.0259 - val_accuracy: 0.8191\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 6s 97us/sample - loss: 0.0287 - accuracy: 0.8040 - val_loss: 0.0257 - val_accuracy: 0.8205\n",
            "============================================================\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 122us/sample - loss: 0.0286 - accuracy: 0.8011 - val_loss: 0.0263 - val_accuracy: 0.8167\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 119us/sample - loss: 0.0230 - accuracy: 0.8421 - val_loss: 0.0224 - val_accuracy: 0.8457\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0214 - accuracy: 0.8536 - val_loss: 0.0212 - val_accuracy: 0.8523\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0207 - accuracy: 0.8587 - val_loss: 0.0243 - val_accuracy: 0.8313\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0200 - accuracy: 0.8652 - val_loss: 0.0212 - val_accuracy: 0.8581\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0195 - accuracy: 0.8674 - val_loss: 0.0202 - val_accuracy: 0.8608\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 7s 124us/sample - loss: 0.0189 - accuracy: 0.8723 - val_loss: 0.0193 - val_accuracy: 0.8694\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0185 - accuracy: 0.8739 - val_loss: 0.0208 - val_accuracy: 0.8595\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0183 - accuracy: 0.8776 - val_loss: 0.0190 - val_accuracy: 0.8709\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0179 - accuracy: 0.8803 - val_loss: 0.0184 - val_accuracy: 0.8740\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 120us/sample - loss: 0.0176 - accuracy: 0.8814 - val_loss: 0.0190 - val_accuracy: 0.8696\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0172 - accuracy: 0.8844 - val_loss: 0.0192 - val_accuracy: 0.8693\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0173 - accuracy: 0.8843 - val_loss: 0.0191 - val_accuracy: 0.8696\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0170 - accuracy: 0.8867 - val_loss: 0.0203 - val_accuracy: 0.8632\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0168 - accuracy: 0.8877 - val_loss: 0.0186 - val_accuracy: 0.8765\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0167 - accuracy: 0.8883 - val_loss: 0.0187 - val_accuracy: 0.8728\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0165 - accuracy: 0.8909 - val_loss: 0.0181 - val_accuracy: 0.8781\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0162 - accuracy: 0.8924 - val_loss: 0.0185 - val_accuracy: 0.8748\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 7s 123us/sample - loss: 0.0161 - accuracy: 0.8923 - val_loss: 0.0197 - val_accuracy: 0.8697\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0160 - accuracy: 0.8945 - val_loss: 0.0198 - val_accuracy: 0.8680\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4e6894780>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwoJN3PvtdwN",
        "colab_type": "text"
      },
      "source": [
        "## 測試initilizer變換\n",
        "    - 排名\n",
        "        - uniform\n",
        "        - defalut"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azi2yo6gtmWK",
        "colab_type": "code",
        "outputId": "8f6d6c35-c442-476e-f82c-d08bc9418ce7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model_init_default = build_model(dropout=True, bn=True)\n",
        "model_init_uniform = build_model(dropout=True, bn=True, init='uniform')\n",
        "\n",
        "# hyper-parameters\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "model_init_default.compile(optimizer=Adam(lr=lr), loss='mse', metrics=['accuracy'])\n",
        "model_init_default.fit(x_train, y_train,\n",
        "                       epochs=epochs,\n",
        "                       batch_size=batch_size,\n",
        "                       validation_data=(x_test, y_test)\n",
        "                       )\n",
        "model_init_uniform.compile(optimizer=Adam(lr=lr), loss='mse', metrics=['accuracy'])\n",
        "model_init_uniform.fit(x_train, y_train,\n",
        "                       epochs=epochs,\n",
        "                       batch_size=batch_size,\n",
        "                       validation_data=(x_test, y_test)\n",
        "                       )"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_23\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_63 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_29 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_40 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_38 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_64 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_30 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_41 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_39 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_65 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_24\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_66 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_31 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_42 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_40 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_43 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_41 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0287 - accuracy: 0.7991 - val_loss: 0.0277 - val_accuracy: 0.8019\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0233 - accuracy: 0.8393 - val_loss: 0.0219 - val_accuracy: 0.8452\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0217 - accuracy: 0.8508 - val_loss: 0.0228 - val_accuracy: 0.8415\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0207 - accuracy: 0.8580 - val_loss: 0.0242 - val_accuracy: 0.8365\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0199 - accuracy: 0.8640 - val_loss: 0.0217 - val_accuracy: 0.8498\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0194 - accuracy: 0.8671 - val_loss: 0.0197 - val_accuracy: 0.8645\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0189 - accuracy: 0.8706 - val_loss: 0.0190 - val_accuracy: 0.8678\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0184 - accuracy: 0.8744 - val_loss: 0.0192 - val_accuracy: 0.8682\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0180 - accuracy: 0.8760 - val_loss: 0.0189 - val_accuracy: 0.8693\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0178 - accuracy: 0.8779 - val_loss: 0.0184 - val_accuracy: 0.8732\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0175 - accuracy: 0.8813 - val_loss: 0.0188 - val_accuracy: 0.8679\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0172 - accuracy: 0.8831 - val_loss: 0.0193 - val_accuracy: 0.8656\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0168 - accuracy: 0.8855 - val_loss: 0.0186 - val_accuracy: 0.8739\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0167 - accuracy: 0.8862 - val_loss: 0.0187 - val_accuracy: 0.8724\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0164 - accuracy: 0.8897 - val_loss: 0.0176 - val_accuracy: 0.8780\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0160 - accuracy: 0.8915 - val_loss: 0.0186 - val_accuracy: 0.8672\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0160 - accuracy: 0.8913 - val_loss: 0.0179 - val_accuracy: 0.8787\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0158 - accuracy: 0.8931 - val_loss: 0.0169 - val_accuracy: 0.8854\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0155 - accuracy: 0.8956 - val_loss: 0.0181 - val_accuracy: 0.8799\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0155 - accuracy: 0.8947 - val_loss: 0.0181 - val_accuracy: 0.8738\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0286 - accuracy: 0.8008 - val_loss: 0.0243 - val_accuracy: 0.8338\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0231 - accuracy: 0.8411 - val_loss: 0.0236 - val_accuracy: 0.8397\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.0217 - accuracy: 0.8495 - val_loss: 0.0224 - val_accuracy: 0.8437\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0207 - accuracy: 0.8586 - val_loss: 0.0216 - val_accuracy: 0.8471\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0199 - accuracy: 0.8644 - val_loss: 0.0205 - val_accuracy: 0.8596\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0194 - accuracy: 0.8676 - val_loss: 0.0192 - val_accuracy: 0.8673\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0189 - accuracy: 0.8707 - val_loss: 0.0198 - val_accuracy: 0.8638\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0183 - accuracy: 0.8746 - val_loss: 0.0215 - val_accuracy: 0.8495\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0179 - accuracy: 0.8776 - val_loss: 0.0196 - val_accuracy: 0.8668\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0177 - accuracy: 0.8784 - val_loss: 0.0181 - val_accuracy: 0.8756\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0173 - accuracy: 0.8817 - val_loss: 0.0200 - val_accuracy: 0.8593\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0172 - accuracy: 0.8829 - val_loss: 0.0210 - val_accuracy: 0.8550\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0168 - accuracy: 0.8853 - val_loss: 0.0184 - val_accuracy: 0.8751\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0165 - accuracy: 0.8872 - val_loss: 0.0177 - val_accuracy: 0.8802\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0162 - accuracy: 0.8897 - val_loss: 0.0196 - val_accuracy: 0.8630\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0162 - accuracy: 0.8902 - val_loss: 0.0187 - val_accuracy: 0.8703\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0159 - accuracy: 0.8924 - val_loss: 0.0199 - val_accuracy: 0.8653\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0158 - accuracy: 0.8927 - val_loss: 0.0186 - val_accuracy: 0.8726\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0156 - accuracy: 0.8946 - val_loss: 0.0182 - val_accuracy: 0.8781\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0156 - accuracy: 0.8944 - val_loss: 0.0189 - val_accuracy: 0.8726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4dcdecf28>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pxWkX43NhJIg",
        "colab_type": "text"
      },
      "source": [
        "# 此處測試一下Dropout 與 BN 哪個先好?\n",
        "- Result: 差不多"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZv5VYtes5-Z",
        "colab_type": "text"
      },
      "source": [
        "測試層數增加\n",
        "- 3層與4層差距不大, 但3層訓練參數少\n",
        "    - 故此情況，利用3層即可。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-03-21T13:31:45.730962Z",
          "start_time": "2020-03-21T13:31:45.710019Z"
        },
        "id": "AuAFH-q3TGp9",
        "colab_type": "code",
        "outputId": "953f72dc-a9bc-41fa-9a17-3cec64661f63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 測試層數參加\n",
        "\n",
        "model_3 = build_model(layers=3, dropout=True, bn=True)\n",
        "model_4 = build_model(layers=4, dropout=True, bn=True)\n",
        "\n",
        "# 超參數\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "# train\n",
        "# keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
        "model_3.compile(optimizer=Adam(lr=lr), loss='mse', metrics=['accuracy'])\n",
        "model_3.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))\n",
        "\n",
        "print('==='*20)\n",
        "\n",
        "model_4.compile(optimizer=Adam(lr=lr), loss='mse', metrics=['accuracy'])\n",
        "model_4.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_50 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_20 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_31 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_29 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_21 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_32 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_20\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_53 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_33 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_34 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 128)               16512     \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_35 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 253,706\n",
            "Trainable params: 252,682\n",
            "Non-trainable params: 1,024\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0285 - accuracy: 0.8005 - val_loss: 0.0243 - val_accuracy: 0.8335\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 6s 105us/sample - loss: 0.0232 - accuracy: 0.8415 - val_loss: 0.0248 - val_accuracy: 0.8289\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0216 - accuracy: 0.8507 - val_loss: 0.0260 - val_accuracy: 0.8165\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0206 - accuracy: 0.8592 - val_loss: 0.0235 - val_accuracy: 0.8409\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0200 - accuracy: 0.8624 - val_loss: 0.0201 - val_accuracy: 0.8619\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0193 - accuracy: 0.8687 - val_loss: 0.0194 - val_accuracy: 0.8665\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 6s 108us/sample - loss: 0.0188 - accuracy: 0.8706 - val_loss: 0.0194 - val_accuracy: 0.8680\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0184 - accuracy: 0.8744 - val_loss: 0.0205 - val_accuracy: 0.8552\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 7s 115us/sample - loss: 0.0180 - accuracy: 0.8768 - val_loss: 0.0214 - val_accuracy: 0.8484\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 7s 118us/sample - loss: 0.0175 - accuracy: 0.8807 - val_loss: 0.0188 - val_accuracy: 0.8720\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0173 - accuracy: 0.8823 - val_loss: 0.0188 - val_accuracy: 0.8707\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0169 - accuracy: 0.8846 - val_loss: 0.0192 - val_accuracy: 0.8676\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0168 - accuracy: 0.8863 - val_loss: 0.0177 - val_accuracy: 0.8809\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.0164 - accuracy: 0.8887 - val_loss: 0.0184 - val_accuracy: 0.8754\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0163 - accuracy: 0.8903 - val_loss: 0.0187 - val_accuracy: 0.8704\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.0159 - accuracy: 0.8919 - val_loss: 0.0191 - val_accuracy: 0.8696\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0158 - accuracy: 0.8935 - val_loss: 0.0177 - val_accuracy: 0.8775\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0155 - accuracy: 0.8946 - val_loss: 0.0171 - val_accuracy: 0.8843\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 7s 110us/sample - loss: 0.0154 - accuracy: 0.8961 - val_loss: 0.0207 - val_accuracy: 0.8605\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 7s 108us/sample - loss: 0.0155 - accuracy: 0.8960 - val_loss: 0.0179 - val_accuracy: 0.8773\n",
            "============================================================\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 9s 153us/sample - loss: 0.0303 - accuracy: 0.7883 - val_loss: 0.0249 - val_accuracy: 0.8280\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0245 - accuracy: 0.8323 - val_loss: 0.0230 - val_accuracy: 0.8401\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 9s 146us/sample - loss: 0.0228 - accuracy: 0.8444 - val_loss: 0.0215 - val_accuracy: 0.8524\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0215 - accuracy: 0.8528 - val_loss: 0.0208 - val_accuracy: 0.8573\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 8s 140us/sample - loss: 0.0207 - accuracy: 0.8583 - val_loss: 0.0210 - val_accuracy: 0.8563\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 9s 144us/sample - loss: 0.0200 - accuracy: 0.8644 - val_loss: 0.0208 - val_accuracy: 0.8571\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 9s 147us/sample - loss: 0.0195 - accuracy: 0.8678 - val_loss: 0.0196 - val_accuracy: 0.8672\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0190 - accuracy: 0.8717 - val_loss: 0.0201 - val_accuracy: 0.8609\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 8s 134us/sample - loss: 0.0187 - accuracy: 0.8728 - val_loss: 0.0201 - val_accuracy: 0.8622\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0183 - accuracy: 0.8763 - val_loss: 0.0194 - val_accuracy: 0.8661\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0178 - accuracy: 0.8785 - val_loss: 0.0188 - val_accuracy: 0.8711\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0175 - accuracy: 0.8818 - val_loss: 0.0209 - val_accuracy: 0.8611\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 9s 147us/sample - loss: 0.0172 - accuracy: 0.8843 - val_loss: 0.0191 - val_accuracy: 0.8726\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 9s 143us/sample - loss: 0.0171 - accuracy: 0.8846 - val_loss: 0.0194 - val_accuracy: 0.8672\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 8s 138us/sample - loss: 0.0170 - accuracy: 0.8856 - val_loss: 0.0188 - val_accuracy: 0.8714\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0166 - accuracy: 0.8887 - val_loss: 0.0183 - val_accuracy: 0.8716\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 8s 136us/sample - loss: 0.0165 - accuracy: 0.8888 - val_loss: 0.0184 - val_accuracy: 0.8718\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 8s 139us/sample - loss: 0.0161 - accuracy: 0.8914 - val_loss: 0.0180 - val_accuracy: 0.8772\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 8s 142us/sample - loss: 0.0159 - accuracy: 0.8936 - val_loss: 0.0172 - val_accuracy: 0.8831\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.0160 - accuracy: 0.8923 - val_loss: 0.0175 - val_accuracy: 0.8819\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4e45cbc50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0j9maT2GyQLJ",
        "colab_type": "text"
      },
      "source": [
        "# Batch測試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1xWM2jfySgd",
        "colab_type": "code",
        "outputId": "c2e39dd9-ea97-4aca-cba9-fb440610759f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# hype-parameters\n",
        "batch_size_50 = 50\n",
        "batch_size_100 = 100\n",
        "batch_size_200 = 200\n",
        "\n",
        "epochs = 15    # 因為太多資料了, 先減少\n",
        "lr = 0.01\n",
        "\n",
        "\n",
        "model_50 = build_model(dropout=True, bn=True)\n",
        "model_100 = build_model(dropout=True, bn=True)\n",
        "model_200 = build_model(dropout=True, bn=True)\n",
        "\n",
        "model_50.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model_50.fit(x_train, y_train,\n",
        "             epochs=epochs,\n",
        "             batch_size=batch_size_50,\n",
        "             validation_data=(x_test, y_test))\n",
        "\n",
        "model_100.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model_100.fit(x_train, y_train,\n",
        "             epochs=epochs,\n",
        "             batch_size=batch_size_100,\n",
        "             validation_data=(x_test, y_test))\n",
        "\n",
        "model_200.compile(optimizer=Adam(learning_rate=lr), loss='mse', metrics=['accuracy'])\n",
        "model_200.fit(x_train, y_train,\n",
        "             epochs=epochs,\n",
        "             batch_size=batch_size_200,\n",
        "             validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_25\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_69 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_44 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_42 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_45 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_43 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_26\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_72 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_35 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_46 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_44 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_73 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_36 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_47 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_45 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_74 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Model: \"sequential_27\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_75 (Dense)             (None, 256)               200960    \n",
            "_________________________________________________________________\n",
            "dropout_37 (Dropout)         (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_48 (Batc (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_46 (Activation)   (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_76 (Dense)             (None, 128)               32896     \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_49 (Batc (None, 128)               512       \n",
            "_________________________________________________________________\n",
            "activation_47 (Activation)   (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_77 (Dense)             (None, 10)                1290      \n",
            "=================================================================\n",
            "Total params: 236,682\n",
            "Trainable params: 235,914\n",
            "Non-trainable params: 768\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 7s 122us/sample - loss: 0.0283 - accuracy: 0.8017 - val_loss: 0.0248 - val_accuracy: 0.8277\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 7s 111us/sample - loss: 0.0233 - accuracy: 0.8393 - val_loss: 0.0250 - val_accuracy: 0.8245\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 7s 113us/sample - loss: 0.0217 - accuracy: 0.8506 - val_loss: 0.0260 - val_accuracy: 0.8246\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0208 - accuracy: 0.8562 - val_loss: 0.0224 - val_accuracy: 0.8443\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0198 - accuracy: 0.8637 - val_loss: 0.0203 - val_accuracy: 0.8587\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0194 - accuracy: 0.8678 - val_loss: 0.0204 - val_accuracy: 0.8585\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 6s 106us/sample - loss: 0.0188 - accuracy: 0.8726 - val_loss: 0.0185 - val_accuracy: 0.8737\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 7s 116us/sample - loss: 0.0185 - accuracy: 0.8744 - val_loss: 0.0208 - val_accuracy: 0.8574\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 7s 112us/sample - loss: 0.0182 - accuracy: 0.8766 - val_loss: 0.0195 - val_accuracy: 0.8675\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0178 - accuracy: 0.8788 - val_loss: 0.0190 - val_accuracy: 0.8710\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.0174 - accuracy: 0.8813 - val_loss: 0.0190 - val_accuracy: 0.8718\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 6s 107us/sample - loss: 0.0172 - accuracy: 0.8834 - val_loss: 0.0193 - val_accuracy: 0.8686\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 7s 109us/sample - loss: 0.0170 - accuracy: 0.8838 - val_loss: 0.0185 - val_accuracy: 0.8742\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 7s 117us/sample - loss: 0.0167 - accuracy: 0.8864 - val_loss: 0.0181 - val_accuracy: 0.8760\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 7s 114us/sample - loss: 0.0163 - accuracy: 0.8891 - val_loss: 0.0180 - val_accuracy: 0.8780\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0272 - accuracy: 0.8111 - val_loss: 0.0247 - val_accuracy: 0.8271\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0220 - accuracy: 0.8487 - val_loss: 0.0228 - val_accuracy: 0.8413\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0202 - accuracy: 0.8603 - val_loss: 0.0220 - val_accuracy: 0.8467\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0191 - accuracy: 0.8681 - val_loss: 0.0200 - val_accuracy: 0.8665\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 3s 58us/sample - loss: 0.0185 - accuracy: 0.8730 - val_loss: 0.0212 - val_accuracy: 0.8557\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0179 - accuracy: 0.8773 - val_loss: 0.0203 - val_accuracy: 0.8628\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 4s 58us/sample - loss: 0.0173 - accuracy: 0.8814 - val_loss: 0.0192 - val_accuracy: 0.8682\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0170 - accuracy: 0.8838 - val_loss: 0.0198 - val_accuracy: 0.8636\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 3s 57us/sample - loss: 0.0167 - accuracy: 0.8856 - val_loss: 0.0189 - val_accuracy: 0.8708\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0165 - accuracy: 0.8875 - val_loss: 0.0198 - val_accuracy: 0.8629\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0161 - accuracy: 0.8901 - val_loss: 0.0173 - val_accuracy: 0.8793\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0160 - accuracy: 0.8920 - val_loss: 0.0181 - val_accuracy: 0.8755\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0153 - accuracy: 0.8964 - val_loss: 0.0181 - val_accuracy: 0.8748\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0153 - accuracy: 0.8970 - val_loss: 0.0201 - val_accuracy: 0.8587\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0150 - accuracy: 0.8987 - val_loss: 0.0188 - val_accuracy: 0.8717\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 2s 37us/sample - loss: 0.0268 - accuracy: 0.8114 - val_loss: 0.0241 - val_accuracy: 0.8279\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0214 - accuracy: 0.8514 - val_loss: 0.0224 - val_accuracy: 0.8427\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0197 - accuracy: 0.8634 - val_loss: 0.0220 - val_accuracy: 0.8474\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0185 - accuracy: 0.8722 - val_loss: 0.0228 - val_accuracy: 0.8467\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0179 - accuracy: 0.8765 - val_loss: 0.0213 - val_accuracy: 0.8524\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 2s 31us/sample - loss: 0.0173 - accuracy: 0.8808 - val_loss: 0.0211 - val_accuracy: 0.8536\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 2s 33us/sample - loss: 0.0166 - accuracy: 0.8859 - val_loss: 0.0200 - val_accuracy: 0.8607\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0164 - accuracy: 0.8884 - val_loss: 0.0185 - val_accuracy: 0.8740\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0159 - accuracy: 0.8921 - val_loss: 0.0204 - val_accuracy: 0.8634\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 2s 35us/sample - loss: 0.0157 - accuracy: 0.8918 - val_loss: 0.0206 - val_accuracy: 0.8562\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0154 - accuracy: 0.8947 - val_loss: 0.0180 - val_accuracy: 0.8762\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0150 - accuracy: 0.8976 - val_loss: 0.0193 - val_accuracy: 0.8673\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 2s 34us/sample - loss: 0.0149 - accuracy: 0.8988 - val_loss: 0.0186 - val_accuracy: 0.8754\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 2s 30us/sample - loss: 0.0145 - accuracy: 0.9018 - val_loss: 0.0178 - val_accuracy: 0.8764\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 2s 32us/sample - loss: 0.0143 - accuracy: 0.9030 - val_loss: 0.0186 - val_accuracy: 0.8719\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff4d6806898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQpjOuUDx72C",
        "colab_type": "text"
      },
      "source": [
        "# CNN 測試"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-03-21T13:31:46.045Z"
        },
        "id": "2jwsJGhETGqB",
        "colab_type": "code",
        "outputId": "4a8886a0-0c87-4abc-eb8e-8d6b2f55f330",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 超參數\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_test_original.shape[1:]))   # (60000, 28, 28, 1) CNN需要RGB通道\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(learning_rate=lr),\n",
        "              metrics=['accuracy'],\n",
        "              )\n",
        "\n",
        "history = model.fit(x_train_original, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test_original, y_test),\n",
        "                    steps_per_epoch=None)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 512)               819712    \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 889,834\n",
            "Trainable params: 889,834\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 13s 220us/sample - loss: 0.4802 - accuracy: 0.8238 - val_loss: 0.3082 - val_accuracy: 0.8872\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 9s 149us/sample - loss: 0.3128 - accuracy: 0.8867 - val_loss: 0.3008 - val_accuracy: 0.8899\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 9s 144us/sample - loss: 0.2861 - accuracy: 0.8985 - val_loss: 0.2737 - val_accuracy: 0.9026\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.2771 - accuracy: 0.9011 - val_loss: 0.2601 - val_accuracy: 0.9073\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 9s 149us/sample - loss: 0.2795 - accuracy: 0.9025 - val_loss: 0.2817 - val_accuracy: 0.8984\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 9s 146us/sample - loss: 0.2822 - accuracy: 0.9026 - val_loss: 0.2660 - val_accuracy: 0.9044\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.2870 - accuracy: 0.9027 - val_loss: 0.2697 - val_accuracy: 0.9049\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 9s 147us/sample - loss: 0.2939 - accuracy: 0.9003 - val_loss: 0.2791 - val_accuracy: 0.9044\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 9s 148us/sample - loss: 0.2957 - accuracy: 0.9000 - val_loss: 0.2603 - val_accuracy: 0.9047\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 9s 146us/sample - loss: 0.3059 - accuracy: 0.8990 - val_loss: 0.2788 - val_accuracy: 0.9023\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.3071 - accuracy: 0.8976 - val_loss: 0.3247 - val_accuracy: 0.8989\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 9s 147us/sample - loss: 0.3125 - accuracy: 0.8966 - val_loss: 0.2796 - val_accuracy: 0.8997\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.3203 - accuracy: 0.8967 - val_loss: 0.4343 - val_accuracy: 0.8849\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.3258 - accuracy: 0.8934 - val_loss: 0.3089 - val_accuracy: 0.8957\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 9s 146us/sample - loss: 0.3339 - accuracy: 0.8913 - val_loss: 0.3034 - val_accuracy: 0.9003\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 9s 146us/sample - loss: 0.3395 - accuracy: 0.8889 - val_loss: 0.3983 - val_accuracy: 0.8847\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 9s 146us/sample - loss: 0.3478 - accuracy: 0.8870 - val_loss: 0.3266 - val_accuracy: 0.8882\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 9s 146us/sample - loss: 0.3521 - accuracy: 0.8870 - val_loss: 0.2943 - val_accuracy: 0.9000\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 9s 145us/sample - loss: 0.3579 - accuracy: 0.8853 - val_loss: 0.2917 - val_accuracy: 0.9004\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 9s 142us/sample - loss: 0.3671 - accuracy: 0.8839 - val_loss: 0.3507 - val_accuracy: 0.8905\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6nDOxKoCTGqE",
        "colab_type": "code",
        "outputId": "89007067-b846-4792-a1c6-fcfa378aac3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 超參數\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_test_original.shape[1:]))   # (60000, 28, 28, 1)\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_original, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test_original, y_test),\n",
        "                    steps_per_epoch=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_51 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_63 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_52 (Conv2D)           (None, 26, 26, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_64 (Activation)   (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_16 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_38 (Dropout)         (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_53 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_65 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_54 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_66 (Activation)   (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_17 (MaxPooling (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_39 (Dropout)         (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_11 (Flatten)         (None, 1600)              0         \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 512)               819712    \n",
            "_________________________________________________________________\n",
            "activation_67 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_40 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_68 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 889,834\n",
            "Trainable params: 889,834\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 11s 176us/sample - loss: 0.0252 - accuracy: 0.8204 - val_loss: 0.0174 - val_accuracy: 0.8792\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 10s 170us/sample - loss: 0.0169 - accuracy: 0.8836 - val_loss: 0.0156 - val_accuracy: 0.8932\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 10s 170us/sample - loss: 0.0150 - accuracy: 0.8986 - val_loss: 0.0139 - val_accuracy: 0.9038\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 10s 170us/sample - loss: 0.0142 - accuracy: 0.9042 - val_loss: 0.0145 - val_accuracy: 0.9015\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 10s 173us/sample - loss: 0.0135 - accuracy: 0.9087 - val_loss: 0.0129 - val_accuracy: 0.9099\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 10s 167us/sample - loss: 0.0131 - accuracy: 0.9125 - val_loss: 0.0142 - val_accuracy: 0.9052\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0127 - accuracy: 0.9154 - val_loss: 0.0125 - val_accuracy: 0.9128\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0124 - accuracy: 0.9182 - val_loss: 0.0125 - val_accuracy: 0.9149\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0121 - accuracy: 0.9208 - val_loss: 0.0126 - val_accuracy: 0.9149\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 10s 167us/sample - loss: 0.0121 - accuracy: 0.9204 - val_loss: 0.0132 - val_accuracy: 0.9137\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 10s 168us/sample - loss: 0.0119 - accuracy: 0.9224 - val_loss: 0.0124 - val_accuracy: 0.9155\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 10s 173us/sample - loss: 0.0116 - accuracy: 0.9238 - val_loss: 0.0121 - val_accuracy: 0.9174\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 10s 171us/sample - loss: 0.0116 - accuracy: 0.9242 - val_loss: 0.0121 - val_accuracy: 0.9198\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0114 - accuracy: 0.9266 - val_loss: 0.0140 - val_accuracy: 0.9100\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0114 - accuracy: 0.9269 - val_loss: 0.0119 - val_accuracy: 0.9216\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 10s 171us/sample - loss: 0.0114 - accuracy: 0.9260 - val_loss: 0.0126 - val_accuracy: 0.9162\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0113 - accuracy: 0.9267 - val_loss: 0.0122 - val_accuracy: 0.9217\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 10s 168us/sample - loss: 0.0110 - accuracy: 0.9287 - val_loss: 0.0121 - val_accuracy: 0.9193\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 10s 167us/sample - loss: 0.0113 - accuracy: 0.9278 - val_loss: 0.0119 - val_accuracy: 0.9249\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 10s 169us/sample - loss: 0.0112 - accuracy: 0.9288 - val_loss: 0.0117 - val_accuracy: 0.9228\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CqkCHQSAuUUG",
        "colab_type": "code",
        "outputId": "5a7309ed-1990-40fc-db69-b451582347ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 超參數\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_test_original.shape[1:]))   # (60000, 28, 28, 1)\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_original, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test_original, y_test),\n",
        "                    steps_per_epoch=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 5, 5, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 5, 5, 64)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 3, 3, 64)          0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 1, 1, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 512)               33280     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 177,258\n",
            "Trainable params: 177,258\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 16s 269us/sample - loss: 0.0333 - accuracy: 0.7564 - val_loss: 0.0225 - val_accuracy: 0.8435\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0210 - accuracy: 0.8565 - val_loss: 0.0187 - val_accuracy: 0.8768\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 11s 187us/sample - loss: 0.0180 - accuracy: 0.8780 - val_loss: 0.0157 - val_accuracy: 0.8929\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 11s 188us/sample - loss: 0.0166 - accuracy: 0.8883 - val_loss: 0.0154 - val_accuracy: 0.8954\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0158 - accuracy: 0.8936 - val_loss: 0.0154 - val_accuracy: 0.9022\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 11s 191us/sample - loss: 0.0152 - accuracy: 0.8982 - val_loss: 0.0149 - val_accuracy: 0.8974\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 11s 188us/sample - loss: 0.0147 - accuracy: 0.9021 - val_loss: 0.0154 - val_accuracy: 0.8995\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0144 - accuracy: 0.9043 - val_loss: 0.0143 - val_accuracy: 0.9090\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0141 - accuracy: 0.9076 - val_loss: 0.0147 - val_accuracy: 0.9037\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 12s 196us/sample - loss: 0.0139 - accuracy: 0.9087 - val_loss: 0.0129 - val_accuracy: 0.9132\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 11s 192us/sample - loss: 0.0141 - accuracy: 0.9090 - val_loss: 0.0135 - val_accuracy: 0.9155\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0138 - accuracy: 0.9090 - val_loss: 0.0135 - val_accuracy: 0.9086\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0136 - accuracy: 0.9119 - val_loss: 0.0127 - val_accuracy: 0.9171\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0136 - accuracy: 0.9107 - val_loss: 0.0135 - val_accuracy: 0.9121\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0138 - accuracy: 0.9111 - val_loss: 0.0133 - val_accuracy: 0.9127\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0136 - accuracy: 0.9126 - val_loss: 0.0133 - val_accuracy: 0.9146\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0136 - accuracy: 0.9120 - val_loss: 0.0136 - val_accuracy: 0.9089\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0135 - accuracy: 0.9144 - val_loss: 0.0133 - val_accuracy: 0.9126\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 11s 189us/sample - loss: 0.0133 - accuracy: 0.9153 - val_loss: 0.0126 - val_accuracy: 0.9154\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 12s 195us/sample - loss: 0.0135 - accuracy: 0.9138 - val_loss: 0.0132 - val_accuracy: 0.9100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVqOAbNsu2gy",
        "colab_type": "code",
        "outputId": "8db7cf0a-4530-44aa-cf88-74ad720916fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 超參數\n",
        "batch_size = 100\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "decay_rate = 0.1\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_test_original.shape[1:], kernel_initializer='uniform'))   # (60000, 28, 28, 1)\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3), kernel_initializer='uniform'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same', kernel_initializer='uniform'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, (3, 3), kernel_initializer='uniform'))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512, kernel_initializer='uniform'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_original, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test_original, y_test),\n",
        "                    steps_per_epoch=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_16\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_43 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_52 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_44 (Conv2D)           (None, 26, 26, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_53 (Activation)   (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_13 (MaxPooling (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_32 (Dropout)         (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_45 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_54 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_46 (Conv2D)           (None, 11, 11, 64)        36928     \n",
            "_________________________________________________________________\n",
            "dropout_33 (Dropout)         (None, 11, 11, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_9 (Flatten)          (None, 7744)              0         \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 512)               3965440   \n",
            "_________________________________________________________________\n",
            "activation_55 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_34 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_56 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 4,035,562\n",
            "Trainable params: 4,035,562\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 12s 196us/sample - loss: 0.0246 - accuracy: 0.8247 - val_loss: 0.0171 - val_accuracy: 0.8807\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 11s 192us/sample - loss: 0.0166 - accuracy: 0.8875 - val_loss: 0.0158 - val_accuracy: 0.8907\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0148 - accuracy: 0.8994 - val_loss: 0.0146 - val_accuracy: 0.9020\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0138 - accuracy: 0.9062 - val_loss: 0.0132 - val_accuracy: 0.9099\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0128 - accuracy: 0.9137 - val_loss: 0.0132 - val_accuracy: 0.9104\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 12s 194us/sample - loss: 0.0122 - accuracy: 0.9191 - val_loss: 0.0123 - val_accuracy: 0.9181\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0117 - accuracy: 0.9217 - val_loss: 0.0121 - val_accuracy: 0.9181\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0112 - accuracy: 0.9255 - val_loss: 0.0123 - val_accuracy: 0.9172\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0109 - accuracy: 0.9288 - val_loss: 0.0126 - val_accuracy: 0.9181\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0108 - accuracy: 0.9293 - val_loss: 0.0123 - val_accuracy: 0.9213\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 12s 198us/sample - loss: 0.0105 - accuracy: 0.9308 - val_loss: 0.0119 - val_accuracy: 0.9242\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0101 - accuracy: 0.9350 - val_loss: 0.0120 - val_accuracy: 0.9237\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 12s 193us/sample - loss: 0.0100 - accuracy: 0.9351 - val_loss: 0.0125 - val_accuracy: 0.9202\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0097 - accuracy: 0.9377 - val_loss: 0.0123 - val_accuracy: 0.9216\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 12s 197us/sample - loss: 0.0097 - accuracy: 0.9388 - val_loss: 0.0121 - val_accuracy: 0.9230\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 11s 190us/sample - loss: 0.0097 - accuracy: 0.9388 - val_loss: 0.0133 - val_accuracy: 0.9185\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 12s 192us/sample - loss: 0.0096 - accuracy: 0.9399 - val_loss: 0.0123 - val_accuracy: 0.9238\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 12s 195us/sample - loss: 0.0099 - accuracy: 0.9395 - val_loss: 0.0135 - val_accuracy: 0.9187\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 12s 195us/sample - loss: 0.0104 - accuracy: 0.9369 - val_loss: 0.0139 - val_accuracy: 0.9138\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 12s 197us/sample - loss: 0.0102 - accuracy: 0.9388 - val_loss: 0.0132 - val_accuracy: 0.9239\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BR6lLm21wL1n",
        "colab_type": "code",
        "outputId": "83877766-b53e-4a2f-9ea7-9bf8d7e0718c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 超參數\n",
        "batch_size = 50\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, (3, 3), padding='same',\n",
        "                 input_shape=x_test_original.shape[1:]))   # (60000, 28, 28, 1)\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(32, (3, 3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "model.add(Conv2D(64, (3, 3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=RMSprop(),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(x_train_original, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test_original, y_test),\n",
        "                    steps_per_epoch=None)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_10 (Conv2D)           (None, 28, 28, 32)        320       \n",
            "_________________________________________________________________\n",
            "activation_13 (Activation)   (None, 28, 28, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_11 (Conv2D)           (None, 26, 26, 32)        9248      \n",
            "_________________________________________________________________\n",
            "activation_14 (Activation)   (None, 26, 26, 32)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "dropout_10 (Dropout)         (None, 13, 13, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_12 (Conv2D)           (None, 13, 13, 64)        18496     \n",
            "_________________________________________________________________\n",
            "activation_15 (Activation)   (None, 13, 13, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_2 (Flatten)          (None, 10816)             0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 512)               5538304   \n",
            "_________________________________________________________________\n",
            "activation_16 (Activation)   (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_11 (Dropout)         (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_17 (Activation)   (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 5,571,498\n",
            "Trainable params: 5,571,498\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 13s 212us/sample - loss: 0.0221 - accuracy: 0.8460 - val_loss: 0.0163 - val_accuracy: 0.8883\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 13s 212us/sample - loss: 0.0147 - accuracy: 0.8997 - val_loss: 0.0138 - val_accuracy: 0.9045\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0129 - accuracy: 0.9125 - val_loss: 0.0124 - val_accuracy: 0.9144\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 13s 212us/sample - loss: 0.0118 - accuracy: 0.9208 - val_loss: 0.0125 - val_accuracy: 0.9164\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0110 - accuracy: 0.9273 - val_loss: 0.0118 - val_accuracy: 0.9201\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 12s 208us/sample - loss: 0.0105 - accuracy: 0.9299 - val_loss: 0.0116 - val_accuracy: 0.9248\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0098 - accuracy: 0.9367 - val_loss: 0.0117 - val_accuracy: 0.9216\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0095 - accuracy: 0.9381 - val_loss: 0.0123 - val_accuracy: 0.9174\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0091 - accuracy: 0.9416 - val_loss: 0.0111 - val_accuracy: 0.9265\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 12s 207us/sample - loss: 0.0090 - accuracy: 0.9422 - val_loss: 0.0118 - val_accuracy: 0.9199\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 13s 211us/sample - loss: 0.0089 - accuracy: 0.9437 - val_loss: 0.0110 - val_accuracy: 0.9264\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0087 - accuracy: 0.9447 - val_loss: 0.0123 - val_accuracy: 0.9189\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0086 - accuracy: 0.9455 - val_loss: 0.0110 - val_accuracy: 0.9268\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 12s 208us/sample - loss: 0.0082 - accuracy: 0.9481 - val_loss: 0.0126 - val_accuracy: 0.9231\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 13s 211us/sample - loss: 0.0082 - accuracy: 0.9489 - val_loss: 0.0120 - val_accuracy: 0.9221\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0080 - accuracy: 0.9504 - val_loss: 0.0121 - val_accuracy: 0.9223\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 13s 209us/sample - loss: 0.0081 - accuracy: 0.9494 - val_loss: 0.0120 - val_accuracy: 0.9218\n",
            "Epoch 18/20\n",
            "60000/60000 [==============================] - 12s 208us/sample - loss: 0.0080 - accuracy: 0.9506 - val_loss: 0.0130 - val_accuracy: 0.9155\n",
            "Epoch 19/20\n",
            "60000/60000 [==============================] - 13s 210us/sample - loss: 0.0081 - accuracy: 0.9506 - val_loss: 0.0132 - val_accuracy: 0.9155\n",
            "Epoch 20/20\n",
            "60000/60000 [==============================] - 13s 211us/sample - loss: 0.0078 - accuracy: 0.9519 - val_loss: 0.0120 - val_accuracy: 0.9236\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsQfk7gjCK0i",
        "colab_type": "code",
        "outputId": "fbce5635-6ecb-4089-8b15-68484a9acefc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 超參數\n",
        "# 沒有BN: 5s 80us/sample - loss: 0.0038 - accuracy: 0.9765 - val_loss: 0.0117 - val_accuracy: 0.9276\n",
        "# 有BN: 7s 120us/sample - loss: 0.0040 - accuracy: 0.9747 - val_loss: 0.0110 - val_accuracy: 0.9317\n",
        "# 有BN+Dense(1024): 8s 129us/sample - loss: 0.0047 - accuracy: 0.9709 - val_loss: 0.0113 - val_accuracy: 0.9329\n",
        "# BN+512+32: 7s 113us/sample - loss: 0.0044 - accuracy: 0.9722 - val_loss: 0.0110 - val_accuracy: 0.9309\n",
        "# 多了1x1 CNN: 8s 131us/sample - loss: 0.0041 - accuracy: 0.9740 - val_loss: 0.0118 - val_accuracy: 0.9277\n",
        "# 比原始還少一層 earlystop在16 : 6s 98us/sample - loss: 0.0062 - accuracy: 0.9594 - val_loss: 0.0124 - val_accuracy: 0.9210\n",
        "\n",
        "# lr=0.01, 超爛: 7s 123us/sample - loss: 0.0249 - accuracy: 0.8738 - val_loss: 0.0254 - val_accuracy: 0.8718\n",
        "# lr=0.002, 8s 133us/sample - loss: 0.0051 - accuracy: 0.9683 - val_loss: 0.0109 - val_accuracy: 0.9321\n",
        "# default還比較好= =\n",
        "\n",
        "# MaxPooling x3, 原本只有一個: 6s 106us/sample - loss: 0.0152 - accuracy: 0.8974 - val_loss: 0.0158 - val_accuracy: 0.8908\n",
        "\n",
        "# 原32-32-64\n",
        "# 32-64-128: 10s 159us/sample - loss: 0.0039 - accuracy: 0.9758 - val_loss: 0.0121 - val_accuracy: 0.9253\n",
        "# 16-32-64: 7s 120us/sample - loss: 0.0038 - accuracy: 0.9762 - val_loss: 0.0108 - val_accuracy: 0.9322\n",
        "# 64-128-256: overfitting : 15s 246us/sample - loss: 0.0025 - accuracy: 0.9850 - val_loss: 0.0108 - val_accuracy: 0.9313\n",
        "\n",
        "batch_size = 100\n",
        "epochs = 20\n",
        "lr = 0.01\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(64, (3, 3), padding='same',               \n",
        "                 input_shape=x_test_original.shape[1:]))   # (60000, 28, 28, 1)  # 原filter=32\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))    # 多的\n",
        "model.add(Conv2D(128, (3, 3)))                  # 原32\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Dropout(0.25))\n",
        "\n",
        "# # 多的~~~\n",
        "# model.add(Conv2D(32, (1, 1), padding='same'))     # 修改\n",
        "# model.add(Activation('relu'))\n",
        "# model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(256, (3, 3), padding='same'))     # 修改, 原64\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "# model.add(MaxPooling2D(pool_size=(2, 2)))     # 多的\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(512))      # 修改\n",
        "model.add(Activation('relu'))\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(10))\n",
        "model.add(Activation('softmax'))\n",
        "model.summary()\n",
        "\n",
        "# callback\n",
        "earlystop = EarlyStopping(monitor=\"val_accuracy\", \n",
        "                          patience=5, \n",
        "                          verbose=1\n",
        "                          )\n",
        "\n",
        "\n",
        "model.compile(loss='mse',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy']\n",
        "              )\n",
        "\n",
        "history = model.fit(x_train_original, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test_original, y_test),\n",
        "                    steps_per_epoch=None,\n",
        "                    callbacks=[earlystop])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_3 (Conv2D)            (None, 28, 28, 64)        640       \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 28, 28, 64)        0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 28, 28, 64)        256       \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 26, 26, 128)       73856     \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 26, 26, 128)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 26, 26, 128)       512       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 13, 13, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 13, 13, 256)       295168    \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 13, 13, 256)       0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 13, 13, 256)       1024      \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 43264)             0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               22151680  \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 10)                5130      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 22,530,314\n",
            "Trainable params: 22,528,394\n",
            "Non-trainable params: 1,920\n",
            "_________________________________________________________________\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/20\n",
            "60000/60000 [==============================] - 16s 259us/sample - loss: 0.0226 - accuracy: 0.8533 - val_loss: 0.0195 - val_accuracy: 0.8707\n",
            "Epoch 2/20\n",
            "60000/60000 [==============================] - 15s 247us/sample - loss: 0.0146 - accuracy: 0.9042 - val_loss: 0.0157 - val_accuracy: 0.8940\n",
            "Epoch 3/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0123 - accuracy: 0.9194 - val_loss: 0.0125 - val_accuracy: 0.9174\n",
            "Epoch 4/20\n",
            "60000/60000 [==============================] - 15s 245us/sample - loss: 0.0105 - accuracy: 0.9305 - val_loss: 0.0113 - val_accuracy: 0.9252\n",
            "Epoch 5/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0093 - accuracy: 0.9388 - val_loss: 0.0137 - val_accuracy: 0.9084\n",
            "Epoch 6/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0083 - accuracy: 0.9456 - val_loss: 0.0126 - val_accuracy: 0.9151\n",
            "Epoch 7/20\n",
            "60000/60000 [==============================] - 15s 245us/sample - loss: 0.0074 - accuracy: 0.9524 - val_loss: 0.0109 - val_accuracy: 0.9292\n",
            "Epoch 8/20\n",
            "60000/60000 [==============================] - 15s 245us/sample - loss: 0.0066 - accuracy: 0.9577 - val_loss: 0.0112 - val_accuracy: 0.9277\n",
            "Epoch 9/20\n",
            "60000/60000 [==============================] - 15s 247us/sample - loss: 0.0057 - accuracy: 0.9634 - val_loss: 0.0105 - val_accuracy: 0.9326\n",
            "Epoch 10/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0051 - accuracy: 0.9677 - val_loss: 0.0110 - val_accuracy: 0.9305\n",
            "Epoch 11/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0045 - accuracy: 0.9715 - val_loss: 0.0110 - val_accuracy: 0.9301\n",
            "Epoch 12/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0043 - accuracy: 0.9730 - val_loss: 0.0105 - val_accuracy: 0.9338\n",
            "Epoch 13/20\n",
            "60000/60000 [==============================] - 15s 244us/sample - loss: 0.0036 - accuracy: 0.9775 - val_loss: 0.0110 - val_accuracy: 0.9297\n",
            "Epoch 14/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0033 - accuracy: 0.9793 - val_loss: 0.0105 - val_accuracy: 0.9329\n",
            "Epoch 15/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0030 - accuracy: 0.9812 - val_loss: 0.0105 - val_accuracy: 0.9336\n",
            "Epoch 16/20\n",
            "60000/60000 [==============================] - 15s 247us/sample - loss: 0.0027 - accuracy: 0.9830 - val_loss: 0.0130 - val_accuracy: 0.9216\n",
            "Epoch 17/20\n",
            "60000/60000 [==============================] - 15s 246us/sample - loss: 0.0025 - accuracy: 0.9850 - val_loss: 0.0108 - val_accuracy: 0.9313\n",
            "Epoch 00017: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}